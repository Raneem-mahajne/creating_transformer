// input data
inputs a token string of length block_size (N)
convert string to token and position embedding matrices (block_size*embedding_size) 

// embed
x = token_embedding + position_embedding

// projection
Q = x * W_Q
K = x * W_K
V = x * W_V

// attention score
attention_matrix = (Q * K.transpose() / sqrt(head_size))

// apply mask - to learn only from previous tokens
attention_matrix = mask(attention_matrix)

// Softmax - get probabilities
attention_weights = softmax(attention_matrix)

// gather values 
output = attention_weights * V
